# -*- coding: utf-8 -*-
"""PROJECT - LSTM BASED NEXT WORD PREDICTION DEEP LEARNING MODEL FOR TEXT GENERATION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pKWR9FVSDG7_Eb1ZulNptugA1njaf-kI

# **PROJECT - LSTM BASED NEXT WORD PREDICTION DEEP LEARNING MODEL FOR TEXT GENERATION**

## **Project Description**
This project focuses on predicting the next word in a sequence using a deep learning model based on Long Short-Term Memory (LSTM) networks. By leveraging the sequential nature of text data, the model generates contextually relevant words, enabling applications like text completion and content generation.

## **Dataset**

We used a text dataset containing structured sentences for training. The dataset was preprocessed through:

Tokenization and text cleaning (removal of special characters and lowercasing).
Preparation of input-output pairs for supervised learning.
One-hot encoding to map words into numerical form.

## **Model Architecture**
The model was implemented using TensorFlow and Keras, featuring:

**Embedding Layer**: Converts words into dense vector representations.

**LSTM Layers**: Captures sequential patterns and long-term dependencies.

**Dense Output Layer**: Predicts the next word using softmax activation.

## Importing Necessary Libraries
"""

import numpy as np
np.random.seed(42)
import tensorflow as tf
tf.random.set_seed(42)

from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, LSTM, Dropout, Activation
from tensorflow.keras.optimizers import RMSprop
import matplotlib.pyplot as plt
import pickle
import heapq

"""## Loading Data"""

import requests

# URL of the text file
url = 'https://raw.githubusercontent.com/harshawardhanchitnis/Project-Machine-Learning-LSTM-Based-Next-Word-Generation-Deep-Learning-Model-For-Text-Generation/b66d3af2e9ae6520ca979e452f84988e5c7a239a/LSTM%20Project%20Text.txt'

# Send an HTTP request to the URL and retrieve the text content
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    text = response.text.lower()
    print('corpus length:', len(text))
else:
    print('Failed to retrieve the text file from the URL.')

"""## Data Processing"""

# Creating a list of unique characters and character indices
character = sorted(list(set(text)))
char_indices = dict((c, i) for i, c in enumerate(character))
indices_char = dict((i, c) for i, c in enumerate(character))
print(f'unique chars: {len(character)}')

"""## Chunking the Data into Sequences"""

# chunk 40 characters with 3 sequences
seq_len = 40
step = 3
sentences = []
next_chars = []
for i in range(0, len(text) - seq_len, step):
    sentences.append(text[i: i + seq_len ])
    next_chars.append(text[i + seq_len])
print(f'num training examples: {len(sentences)}')

"""## **One-Hot Encoding of the Data**"""

# One-hot encode the data
X = np.zeros((len(sentences), seq_len, len(character)), dtype=bool)  # Use built-in bool instead of np.bool
y = np.zeros((len(sentences), len(character)), dtype=bool)  # Use built-in bool instead of np.bool

# One-hot encoding process
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        X[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1

"""## **Creating the Model**"""

# Create the LSTM-based model
model = Sequential()
model.add(LSTM(128, input_shape=(seq_len, len(character))))
model.add(Dense(len(character)))
model.add(Activation('softmax'))

# Summarize the model architecture
model.summary()

# Plot the model architecture
from tensorflow.keras.utils import plot_model
plot_model(model, show_shapes=True, to_file='model.png')

"""## Training the Model"""

# Compile and train the model
optimizer = RMSprop(learning_rate=0.01)
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
history = model.fit(X, y,
                    validation_split=0.01,
                    batch_size=128,
                    epochs=30,
                    shuffle=True).history

# Saving the model and history
model.save('keras_model.h5')
pickle.dump(history, open("history.p", "wb"))

# Loading the model and history
model = load_model('keras_model.h5')
history = pickle.load(open("history.p", "rb"))

"""##  Model Evaluation"""

# Evaluate the model
loss_and_acc = model.evaluate(X, y)
print("Test Loss", loss_and_acc[0])
print("Test Accuracy", loss_and_acc[1])

# Plot training & validation accuracy and loss
acc = history['accuracy']
val_acc = history['val_accuracy']
loss = history['loss']
val_loss = history['val_loss']
epochs = range(len(acc))

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))
ax[0].plot(epochs, acc, 'r', label='Training accuracy')
ax[0].plot(epochs, val_acc, 'b', label='Validation accuracy')
ax[0].legend(loc=0)
ax[1].plot(epochs, loss, 'r', label='Training loss')
ax[1].plot(epochs, val_loss, 'b', label='Validation loss')
ax[1].legend(loc=0)

plt.suptitle('Training and validation')
plt.show()

"""## Preparing the Input for Prediction"""

# Prepare the input text for the model
def prepare_input(text):
    x = np.zeros((1, seq_len, len(character)))
    for t, char in enumerate(text):
        x[0, t, char_indices[char]] = 1.
    return x

"""## Sampling Function for Prediction"""

# Sample function to predict based on the model output
def sample(preds, top_n=3):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds)
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    return heapq.nlargest(top_n, range(len(preds)), preds.take)

"""## Predicting Text Completion"""

# Predict completion of the given text
def predict_completion(text):
    original_text = text
    generated = text
    completion = ''
    while True:
        x = prepare_input(text)
        preds = model.predict(x, verbose=0)[0]
        next_index = sample(preds, top_n=1)[0]
        next_char = indices_char[next_index]
        text = text[1:] + next_char
        completion += next_char

        if len(original_text + completion) + 2 > len(original_text) and next_char == ' ':
            return completion

"""## Predicting Multiple Completions"""

def predict_completions(text, n=3, temperature=1.0):
    x = prepare_input(text)
    preds = model.predict(x, verbose=0)[0]

    # Adjust predictions using the temperature
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds + 1e-7) / temperature  # Add small value to avoid log(0)
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)

    # Get the top n predictions
    next_indices = heapq.nlargest(n, range(len(preds)), preds.take)
    return [indices_char[idx] + predict_completion(text[1:] + indices_char[idx]) for idx in next_indices]

"""## Testing Predictions"""

# Example input sequences for predictions
quotes = [
    "Deep learning is subset of machine learning,which is essentially a neural network with three or more layers.",
    "Machine learning  is the study of computer algorithms that improve automatically through experience and by the use of data.",
    "It is not a lack of love, but a lack of friendship that makes unhappy marriages.",
    "Recurrent neural network (RNN) is a type of artificial neural network which uses sequential data or time series data.",
    "Thanks for spending your precious time to view my work."
]

# Generating predictions for each input sequence
for q in quotes:
    seq = q[:40].lower()  # Use the first 40 characters for prediction
    print("Input Sequence: ", seq)
    completions = predict_completions(seq, n=5, temperature=0.7)  # Add temperature parameter
    for i, completion in enumerate(completions, 1):
        print(f"Prediction {i}: {completion}")
    print()

"""# **SUMMARY :**
## **This project focused on building an LSTM-based text generation model to predict text sequences. We preprocessed the data, implemented one-hot encoding, and created training sequences. The LSTM model, trained with categorical cross-entropy loss and RMSprop optimizer, demonstrated its ability to learn patterns in the text.**

# **CLOSING REMARK :**
## **This project showcased the power of LSTM networks in natural language processing. Despite some limitations, it provided a strong foundation for understanding text generation tasks. Future improvements, such as using larger datasets and advanced tuning, could make the model more robust and versatile for practical applications like chatbots and automated writing tools.**
"""